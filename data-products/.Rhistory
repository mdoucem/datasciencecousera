getwd()
getwd()
Sys.getenv('PATH')
system('g++ -v')
system('where make')
system('g++ -v')
getwd()
Sys.getenv('PATH')
system('g++ -v')
system('where make')
install.packages("waffle")
library(waffle)
install.packages("devtools")
devtools::install_version("devtools", version = "1.11.1", repos = "http://cran.us.r-project.org")
survey_data<-read.table("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
head(survey_data)
str(survey_data)
survey_data<-read.cvs("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
str(survey_data)
survey_data<-read.csv("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
str(survey_data)
str(survey_data$VAL)
survey_data_1m<-survey_data[survey_data$VAL == 24]
survey_data_1m
survey_data_1m<-survey_data[survey_data$VAL == '24']
survey_data_1m
survey_data_1m<-which(survey_data$VAL == '24')
survey_data_1m
length(survey_data_1m)
head(survey_data$FES, 15)
install.packages('XLConnect')
library(XLConnect)
excel_data<-readWorksheetFromFile("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx")
head(excel_data)
install.packages('xlsx')
library(xlsx)
excel_data<-read.xlsx("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx",
sheetIndex = 1)
head(excel_data)
library(xlsx)
Sys.getenv('JAVA_HOME')
install.packages('xlsx')
library(xlsx)
library(datasets)
data(africa)
install.packages("faraway")
library(faraway)
data(africa)
data("africa")
setwd("C:/Users/munezero/Documents/Personal-ventures/Courses/R_Cousera/practical-machine-learning")
getwd()
library(ggplot2)
library(caret)
library(knitr)
train_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_des<-"train_set.csv"
test_des<-"test_set.csv"
if (!file.exists(train_des) & !file.exists(test_des)){
trainfile<-download.file(train_url, train_des, method = "libcurl")
testfile<-download.file(test_url, test_des, method = "libcurl")
}
trainset<-read.csv("train_set.csv", sep = ",", stringsAsFactors = FALSE)
str(trainset)
summary(trainset)
require(devtools)
install_github("slidify", "ramnathv")
install_github("slidify", "ramnathv/slidify")
install_github("slidifyLibraries", "ramnathv")
library(slidify)
getwd()
setwd("C:/Users/munezero/Documents/Personal-ventures/Courses")
author("mydeck")
slidify("index.Rmd")
setwd("C:/Users/munezero/Documents/Personal-ventures/Courses/R_Cousera/practical-machine-learning")
head(trainset)
str(trainset)
missing<-apply(trainset, 2, is.na)
head(missing)
sum(missing)
sum(missing[1])
sum(missing[,1])
sum(missing[[1]])
missing[1]
sum(is.na(trainset))
ls()
setwd("C:/Users/munezero/Documents/Personal-ventures/blog-munezero/Oscar-diversity-update-18")
# Load the libraries
library(ggplot2)
library(dplyr)
oscar_df<-read.csv("oscar-diversity.csv", header = TRUE, sep=";", na.strings = "NA", stringsAsFactors = FALSE)
oscar_df$Category<-tolower(oscar_df$Category)
oscar_df<-as.data.frame(apply(oscar_df, 2, as.factor))
df3<-aggregate(Name ~ Year + Category + Race, data = oscar_df, function(X) length(unique(X)))
df2<-subset(df3, Race == "Black")
g<-ggplot(df2, aes(x = Category, y = as.numeric(Name), fill = Year)) +
geom_bar(position= "dodge", stat = "identity") +
labs(x= "Category", y = "Total Nominees", title = "Number of people of color nominated in 5 categories")
library(ggplot2)
remove.packages(c("ggplot2", "data.table"))
install.packages("Rcpp", dependencies = TRUE)
install.packages("ggplot2", dependencies = TRUE)
install.packages("data.table", dependencies = TRUE)
library(ggplot2)
library(dplyr)
g<-ggplot(df2, aes(x = Category, y = as.numeric(Name), fill = Year)) +
geom_bar(position= "dodge", stat = "identity") +
labs(x= "Category", y = "Total Nominees", title = "Number of people of color nominated in 5 categories")
png("df2.png", width=480, height=480)
g + theme(text = element_text(size=15), axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
scale_fill_manual(values=c('#999999','#E69F00'))
#scale_fill_brewer(palette="Blues")
dev.off()
png("d_df1.png", width=480, height=480)
ggplot(d_df1, aes(x = Year, y = as.numeric(Name), fill = Gender)) +
geom_bar(position= "dodge", stat = "identity") +
labs(x = "Year", y = "Total nominees",
title = "Number Males and Females nominated in Best Director category") +
scale_fill_manual(values=c('#999999','#91006f'))
dev.off()
d_df<-subset(oscar_df, Category == "best director")
d_df1<-aggregate(Name ~ Year + Gender, data = d_df, function(X) length(unique(X)))
png("d_df1.png", width=480, height=480)
ggplot(d_df1, aes(x = Year, y = as.numeric(Name), fill = Gender)) +
geom_bar(position= "dodge", stat = "identity") +
labs(x = "Year", y = "Total nominees",
title = "Number Males and Females nominated in Best Director category") +
scale_fill_manual(values=c('#999999','#91006f'))
dev.off()
png("d_df1.png", width=480, height=480)
ggplot(d_df1, aes(x = Year, y = as.numeric(Name), fill = Gender)) +
geom_bar(position= "dodge", stat = "identity") +
labs(x = "Year", y = "Total nominees",
title = "Number Males and Females nominated in Best Director category") +
scale_fill_manual(values=c('#91006f', '#999999'))
dev.off()
setwd("C:/Users/munezero/Documents/Personal-ventures/blog-munezero/Shark-tank")
getwd()
library(urltools)
library(rvest)
library(curl)
df<-read.csv("shark_tank-part2.csv", header = TRUE, sep = ";", stringsAsFactors = FALSE)
df<-read.csv("shark-tank-part2.csv", header = TRUE, sep = ";", stringsAsFactors = FALSE)
getURL <- function(name){
url = URLencode(paste0("http://gazettereview.com/?s=", name))
#page <- read_html(url)
page<-read_html(curl(url, handle = curl::new_handle("useragent"="Chrome")))
results <- page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
result<-results[grep("^....update.*", results)]
# only return the numerals
# Return results if you want to see them all.
return(result)
}
getURL("Biem")
install.packages("xml2")
library(rvest)
getURL <- function(name){
url = URLencode(paste0("http://gazettereview.com/?s=", name))
#page <- read_html(url)
page<-read_html(curl(url, handle = curl::new_handle("useragent"="Chrome")))
results <- page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
result<-results[grep("^....update.*", results)]
# only return the numerals
# Return results if you want to see them all.
return(result)
}
getURL("Biem")
getURL("Wispots")
getURL <- function(name){
url = URLencode(paste0("http://gazettereview.com/?s=", name))
#page <- read_html(url)
page<-read_html(curl(url, handle = curl::new_handle("useragent"="Chrome")))
print(page)
results <- page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
print(results)
result<-results[grep("^....Update.*", results)]
print(result)
# only return the numerals
# Return results if you want to see them all.
return(result)
}
getURL("Wispots")
getURL <- function(name){
url = URLencode(paste0("http://gazettereview.com/?s=", name))
print(name, url)
#page <- read_html(url)
page<-read_html(curl(url, handle = curl::new_handle("useragent"="Chrome")))
print(page)
results <- page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
print(results)
result<-results[grep("^....Update.*", results)]
print(result)
# only return the numerals
# Return results if you want to see them all.
return(result)
}
getURL("Wispots")
getURL <- function(name){
url = URLencode(paste0("http://gazettereview.com/?s=", name))
print(name)
print(url)
#page <- read_html(url)
page<-read_html(curl(url, handle = curl::new_handle("useragent"="Chrome")))
print(page)
results <- page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
print(results)
result<-results[grep("^....Update.*", results)]
print(result)
# only return the numerals
# Return results if you want to see them all.
return(result)
}
getURL("Wispots")
getURL <- function(name){
url = URLencode(paste0("http://gazettereview.com/?s=", name))
print(name)
print(url)
#page <- read_html(url)
page<-read_html(curl(url, handle = curl::new_handle("useragent"="Chrome")))
print(page)
page-url<-paste(xpathSApply(page,
"@itemprop='url']", xmlValue), collapse='|')
print(page)
results <- page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
print(results)
result<-results[grep("^....Update.*", results)]
print(result)
# only return the numerals
# Return results if you want to see them all.
return(result)
}
getURL("Wispots")
require(XML)
require(pbapply)
require(httr)
getURL <- function(name){
url = URLencode(paste0("http://gazettereview.com/?s=", name))
print(name)
print(url)
#page <- read_html(url)
page<-read_html(curl(url, handle = curl::new_handle("useragent"="Chrome")))
print(page)
page-url<-paste(xpathSApply(page,
"@itemprop='url']", xmlValue), collapse='|')
print(page)
results <- page %>%
html_nodes("cite") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
print(results)
result<-results[grep("^....Update.*", results)]
print(result)
# only return the numerals
# Return results if you want to see them all.
return(result)
}
getURL("Wispots")
getURL <- function(name){
url = URLencode(paste0("http://gazettereview.com/?s=", name))
print(name)
print(url)
#page <- read_html(url)
page<-read_html(curl(url, handle = curl::new_handle("useragent"="Chrome")))
print(page)
results <- page %>%
html_nodes("meta itemprop='url'") %>% # Get all notes of type cite. You can change this to grab other node types.
html_text()
print(results)
result<-results[grep("^....Update.*", results)]
print(result)
# only return the numerals
# Return results if you want to see them all.
return(result)
}
getURL("Wispots")
rm(list = ls())
setwd("C:/Users/munezero/Documents/Personal-ventures/Courses/R_Cousera/practical-machine-learning")
library(ggplot2)
library(caret)
library(knitr)
train_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_des<-"train_set.csv"
test_des<-"test_set.csv"
if (!file.exists(train_des) & !file.exists(test_des)){
trainfile<-download.file(train_url, train_des, method = "libcurl")
testfile<-download.file(test_url, test_des, method = "libcurl")
}
trainset<-read.csv("train_set.csv", sep = ",", stringsAsFactors = FALSE)
trainset<-read.csv("train_set.csv", sep = ",", stringsAsFactors = FALSE, na.strings = c("NA","#DIV/0!",""))
trainset<-read.csv("test_set.csv", sep = ",", stringsAsFactors = FALSE, na.strings = c("NA","#DIV/0!",""))
dim(trainset)
trainset<-read.csv("train_set.csv", sep = ",", stringsAsFactors = FALSE, na.strings = c("NA","#DIV/0!",""))
testset<-read.csv("test_set.csv", sep = ",", stringsAsFactors = FALSE, na.strings = c("NA","#DIV/0!",""))
dim(trainset)
dim(testset) #20 * 160
sum(is.na(trainset))
zero_values<-nearZeroVar(trainset, saveMetrics = TRUE)
trainset<-trainset[, -zero_values]
class(trainset)
class(zero_values)
trainset<-trainset[, -unlist(zero_values)]
dim(trainset)
testset<-testset[, -unlist(zero_values)]
dim(testset)
missing_values<-sapply(trainset, function(x) mean(is.na(x))) >0.95
trainset<-trainset[, missing_values == FALSE]
testset<-testset[, missing_values == FALSE]
dim(trainset)
dim(testset)
str(trainset)
levels(trainset$classe)
corMatrix <- cor(trainset[,-44]) # remove the predictor column
corrplot(corMatrix, order = "FPC", method = "color", type = "lower",
tl.cex = 0.8, tl.col = rgb(0,0,0)) # plot a correlation matrix
install.packages("corrplot")
library(corrplot)
corrplot(corMatrix, order = "FPC", method = "color", type = "lower",
tl.cex = 0.8, tl.col = rgb(0,0,0)) # plot a correlation matrix
inTrain<-createDataPartition(trainset, p =0.8, list = FALSE)
inTrain<-createDataPartition(trainset, p =0.7, list = FALSE)
inTrain<-createDataPartition(trainset$classe, p =0.8, list = FALSE)
training<-trainset[inTrain,]
validation<-trainset[-inTrain,]
dim(training)
dim(validation)
control<-trainControl(method = "cv", number=10)
metric<-"Accuracy"
set.seed(7) # should test different seeds as well
fit.lda<-train(as.factor(classe)~., data = training, method="lda", metric=metric, trControl=control)
summary(fit.lda)
predictions<-predict(fit.lda, validation)
predictions
cmtrx<-confusionMatrix(predictions, validation$classe)
cmtrx
plot(fit.lda)
library(mlbench)
importance<-varImp(fit.lda, scale = FALSE)
set.seed(7)
fit.cart<-train(as.factor(Deal)~., data = tank_subset, method="rpart", metric=metric, trControl=control)
set.seed(7)
fit.cart<-train(as.factor(classe)~., data = training, method="rpart", metric=metric, trControl=control)
set.seed(7)
fit.cart<-train(as.factor(classe)~., data = training, method="knn", metric=metric, trControl=control)
set.seed(7)
fit.cart<-train(as.factor(classe)~., data = training, method="svmRadial", metric=metric, trControl=control)
results<-resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn,
svm=fit.svm, rf=fit.rf))
set.seed(7) # should test different seeds as well
fit.lda<-train(as.factor(classe)~., data = training, method="lda", metric=metric, trControl=control)
set.seed(7)
fit.cart<-train(as.factor(classe)~., data = training, method="rpart", metric=metric, trControl=control)
set.seed(7)
fit.knn<-train(as.factor(classe)~., data = training, method="knn", metric=metric, trControl=control)
set.seed(7)
fit.svm<-train(as.factor(classe)~., data = training, method="svmRadial", metric=metric, trControl=control)
set.seed(7)
fit.rf<-train(as.factor(classe)~., data = training, method="rf", metric=metric, trControl=control)
results<-resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn,
svm=fit.svm, rf=fit.rf))
summary(results)
dotplot(results)
summary(fit.rf)
predictions<-predict(fit.rf, validation)
head(predictions, 10) # look at some of the predictions
cmtrx<-confusionMatrix(predictions, validation$classe)
cmtrx
plot(fit.rf)
library(mlbench)
importance<-varImp(fit.rf, scale = FALSE)
print(importance)
plot(importance)
testpredictions<-predict(fit.rf, testset)
cmtrx2<-confusionMatrix(testpredictions, testset$classe)
table(testpredictions)
testpredictions
summary(results) # This plots the accuracy and kappa values of all the models
predictions<-predict(fit.svm, validation)
predictionsRf<-predict(fit.rf, validation)
predictionsSvm<-predict(fit.svm, validation)
cmtrx1<-confusionMatrix(predictionsRf, validation$classe)
cmtrx2<-confusionMatrix(predictionsSvm, validation$classe)
cmtrx1
cmtrx2
plot(fit.rf) # what does this plot show?
plot(fit.svm)
knit2html("pml_course_project.Rmd")
render("pml_course_project.Rmd", "html_document")
library(rmarkdown)
render("pml_course_project.Rmd", "html_document")
install.packages("leaflet")
mymap<- leaflet() %>%
addTiles() %>%
addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")
library(leaflet)
mymap<- leaflet() %>%
addTiles() %>%
addMarkers(lng=174.768, lat=-36.852, popup="The birthplace of R")
mymap
mymap<- leaflet() %>%
addTiles() %>%
addCircles(lng=174.768, lat=-36.852, popup="The birthplace of R")
mymap
mymap<- leaflet() %>%
addTiles() %>%
addCircles(lng=174.768, lat=-36.852, popup="The birthplace of R", text = "hello")
mymap<- leaflet() %>%
addTiles() %>%
addCircles(lng=174.768, lat=-36.852, popup="The birthplace of R", label = "hello")
mymap
mymap<- leaflet() %>%
addTiles() %>%
addCircles(lng=174.768, lat=-36.852, label = "hello")
mymap
mymap<- leaflet() %>%
addTiles() %>%
addLabelOnlyMarkers(lng=174.768, lat=-36.852, label = "hello")
mymap
knitr::opts_chunk$set(echo = FALSE)
df<-data.frame(Latitude = c(52.47867, 51.50642, 53.79480, 53.47959),
Longitude = c(-1.90848, -0.12721, -1.54653, -2.24874),
Labels = c("pl1", "pl2", "pl3", "pl4")
df
df<-data.frame(Latitude = c(52.47867, 51.50642, 53.79480, 53.47959),
Longitude = c(-1.90848, -0.12721, -1.54653, -2.24874))
df
df<-data.frame(Latitude = c(52.47867, 51.50642, 53.79480, 53.47959),
Longitude = c(-1.90848, -0.12721, -1.54653, -2.24874),
Labels = c("pl1", "pl2", "pl3", "pl4"))
df
mymap<- df %>%
leaflet() %>%
addTiles() %>%
addCircles(Latitude, Longitude, popup = Labels)
mymap<- df %>%
leaflet() %>%
addTiles() %>%
addCircles(lat = ~Latitude, lng = ~Longitude, popup = Labels)
mymap<- df %>%
leaflet() %>%
addTiles() %>%
addCircles(lat = ~Latitude, lng = ~Longitude, popup = df$Labels)
mymap
df<-data.frame(Latitude = c(1.9403, 6.3690, 0.0236, 22.9576, 61.9241),
Longitude = c(29.8739, 34.8888, 37.9062, 18.4904, 25.7482),
Labels = c("pl1", "pl2", "pl3", "pl4", "pl5"))
mymap<- df %>%
leaflet() %>%
addTiles() %>%
addMarkers(lat = ~Latitude, lng = ~Longitude, popup = df$Labels)
mymap
library(knitr)
library(rmarkdown)
getwd()
setwd("C:/Users/munezero/Documents/Personal-ventures/Courses/R_Cousera/data-products")
render("mymap.Rmd", "all")
render("mymap.Rmd", "all")
render("mymap.Rmd", "all")
